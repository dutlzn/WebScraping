{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json \n",
    "import urllib\n",
    "import re\n",
    "root_dir = 'D:\\scrapyfile'\n",
    "'''\n",
    "爬取糗事百科中图片\n",
    "'''\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://pic.qiushibaike.com/system/pictures/12415/124156185/medium/S0QSW2LLWZHZ7N8F.jpg'\n",
    "'''\n",
    "content 二进制\n",
    "json 对象\n",
    "text 字符串\n",
    "'''\n",
    "img_data = requests.get(url=url).content\n",
    "\n",
    "with open('D:\\scrapyfile\\qiutu.jpg', 'wb') as f: \n",
    "    f.write(img_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://pic.qiushibaike.com/system/pictures/12415/124156185/medium/S0QSW2LLWZHZ7N8F.jpg'\n",
    "urllib.request.urlretrieve(img_url, 'D:\\scrapyfile\\qiutu.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用urllib的方式爬取图片 无法进行UA封装，而requests的方式可以\n",
    "\n",
    "\n",
    "* 如果在进行数据解析的时候，一定是需要对页面布局进行分析 如果当前网站没有动态加载的数据就可以直接使用Elements对页面布局进行分析，否则只可以使用networks对页面数据进行分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捕获当前首页的页面源码数据\n",
    "url = 'http://www.521609.com/tuku/mxxz/'\n",
    "page_text = requests.get(url = url, headers = headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <li style=\"position: absolute; left: 0px; top: 0px;\"> \n",
    "#     <a href=\"/tuku/1221.html\" title=\"Angelababy变身复古少女漫步古堡 车厘子红唇妆显别样风情\">\n",
    "#         <img src=\"/d/file/p/2021/03-09/6e7addfae4a6507b80bfc3f853a6ea3b.jpg\" alt=\"Angelababy变身复古少女漫步古堡 车厘子红唇妆显别样风情\">\n",
    "#         <p>Angelababy变身复古少女漫步古堡 车厘子红唇妆显别样风情</p>\n",
    "#     </a>\n",
    "# </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则表达式教程\n",
    "[https://www.runoob.com/regexp/regexp-syntax.html](参考链接)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import bs4\n",
    "dirName = 'D:\\scrapyfile\\ImgLibs'\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    \n",
    "ex = '<img src=\"(.*)\" alt=.*>'\n",
    "img_src_list = re.findall(ex, page_text) # re.S 处理换行\n",
    "for src in img_src_list:\n",
    "    src = 'http://www.521609.com/' + src \n",
    "    imgPath = dirName + '/' + src.split('/')[-1]\n",
    "    urllib.request.urlretrieve(src, imgPath)\n",
    "    print(\"下载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS4\n",
    "\n",
    "* 数据解析的原理：指定标签的定位\n",
    "* 数据解析的原理：取出标签中存储的数据或者标签属性中的数据\n",
    "\n",
    "- BeautifulSoup对象的实例化\n",
    "    - BeautifulSoup(fp, 'lxml') 用来将本地存储的html文档中的数据进行解析\n",
    "    - BeautifulSoup(page_text, 'lxml') 用来将互联网上请求到的页面源码数据进行解析\n",
    "   \n",
    "- 标签定位：\n",
    "    - soup.tagName: 只可以定位到第一次出现的tagName标签\n",
    "    - soup.find('tagName', attrName='value') 属性定位\n",
    "    - soup.findAll: 跟find一样用作属性定位，只不过findAll返回的是列表\n",
    "    - soup.select() 选择器\n",
    "        - 类选择器\n",
    "        - id选择器\n",
    "        - 层级选择器\n",
    "            - >:表示一个层级\n",
    "            - 空格: 表示多个层级\n",
    "            \n",
    "- 取数据:\n",
    "    - .text 返回的是该标签下所有的文本内容\n",
    "    - .string 返回的是该标签直系的文本内容\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS4实战\n",
    "\n",
    "爬取三篇全篇内容：[三国演义](https://www.shicimingju.com/book/sanguoyanyi.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "page_url = requests.get(url=main_url, headers=headers).text\n",
    "from bs4 import BeautifulSoup\n",
    "# 数据解析： 章节标题、详情url、章节内容\n",
    "html = urlopen(main_url)\n",
    "soup = BeautifulSoup(html)\n",
    "a_list = soup.select('.book-mulu > ul > li > a')\n",
    "print(len(a_list))\n",
    "fp = open('D:\\scrapyfile\\sanguo.txt','w',encoding='utf-8')\n",
    "for a in a_list:\n",
    "    title = a.string \n",
    "    detail_url = 'https://www.shicimingju.com/' + a['href']\n",
    "    html = urlopen(detail_url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    content = soup.find('div', class_='chapter_content').text\n",
    "    fp.write(title+\":\"+content+\"\\n\")\n",
    "    print(title+\"\\t 已经保存完毕\")\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xpath\n",
    "- 环境安装\n",
    "    - pip install lxml\n",
    "- 解析原理： html标签是以树状形式进行展示\n",
    "    - 1.实例化一个etree的对象，且将待解析的页面源码数据加载到该对象中\n",
    "    - 2.调用etree对象的xpath方法，结合着不同的xpath表达式实现标签的定位和数据提取\n",
    "- 实例化etree对象\n",
    "    - etree.parse('fileName'):将本地html文档加载到该对象中\n",
    "    - etree.HTML('page_text'):网站获取的页面数据加载到该对象中\n",
    "- 标签定位\n",
    "    - 最左边/: 如果xpath表达式最左侧是以/开头则表示该表达式一定要从根标签开始定位指定标签\n",
    "    - 非最左侧的/:表示一个层级\n",
    "    - 非最左侧的//： 表示多个层级\n",
    "    - 最左侧的//: xpath表达式可以从任意位置进行标签定位\n",
    "    - 属性定位: tagName[@attrName=\"value\"]\n",
    "    - 索引定位: tag[index] 索引从1开始的\n",
    "    - 模糊匹配:\n",
    "        - //div[contains(@class, \"ng\")]\n",
    "        - //div[starts-with(@class, \"ta\")]\n",
    "- 获取文本\n",
    "    - /text(): 直系文本内容\n",
    "    - //text():所有的文本内容\n",
    "- 获取属性\n",
    "    - /@attrName \n",
    "- 局部数据解析：\n",
    "    - 我们要将定位到的页面中的标签作为待解析的数据，再次使用xpath表达式解析待解析的数据\n",
    "    - 在局部数据解析的时候，xpath表达式中要使用./的操作，./表示的就是当前的局部数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "img_dir = 'D:/scrapyfile/img1'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "url = 'https://pic.netbian.com/4kmeinv/'\n",
    "response  = requests.get(url=url, headers=headers)\n",
    "response.encoding = 'gbk'\n",
    "page_text = response.text\n",
    "tree = etree.HTML(page_text)\n",
    "li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "for li in li_list:\n",
    "#     print(type(li)) li的数据类型和tree的数据类型一样，li也可以调用xpath的方法\n",
    "    title = li.xpath('./a/img/@alt')[0] + '.jpg'\n",
    "    img_src = 'https://pic.netbian.com' + li.xpath('./a/img/@src')[0]\n",
    "    img_data = requests.get(url=img_src, headers=headers).content\n",
    "    img_path = img_dir + '/' + title\n",
    "    with open(img_path, 'wb') as fp:\n",
    "        fp.write(img_data)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分页爬取\n",
    "img_dir = 'D:/scrapyfile/img2'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "url = 'https://pic.netbian.com/4kmeinv/index_%d.html'\n",
    "for page in range(1,6):\n",
    "    if page == 1:\n",
    "        new_page = 'https://pic.netbian.com/4kmeinv'\n",
    "    else:   \n",
    "        new_page = format(url % page)\n",
    "    response = requests.get(url = new_page, headers = headers)\n",
    "    response.encoding = 'gbk'\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "    for li in li_list:\n",
    "        title = li.xpath('./a/img/@alt')[0] + '.jpg'\n",
    "        img_src = 'https://pic.netbian.com' + li.xpath('./a/img/@src')[0]\n",
    "        img_data = requests.get(url=img_src, headers=headers).content\n",
    "        img_path = img_dir + '/' + title\n",
    "        with open(img_path, 'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将https://www.aqistudy.cn/historydata/ 所有城市名称解析出来\n",
    "import requests \n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.aqistudy.cn/historydata/'\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "page_text = requests.get(url = url, headers=headers).text\n",
    "tree = etree.HTML(page_text)\n",
    "hot_cities = tree.xpath('//div[@class=\"bottom\"]/ul/li/a/text()')\n",
    "all_cities = tree.xpath('//div[@class=\"bottom\"]/ul/div[2]/li/a/text()')\n",
    "# tree.xpath('//div[@class=\"bottom\"]/ul/li/a/text() | //div[@class=\"bottom\"]/ul/div[2]/li/a/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战一： 站长素材 下载高清图片\n",
    "import requests \n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "\n",
    "dirName = 'D:\\scrapyfile\\shuaiqi'\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    \n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "url_template = 'https://sc.chinaz.com/tag_tupian/baqi_%d.html' \n",
    "\n",
    "for page in range(1,6):\n",
    "    if page == 1:\n",
    "        url = 'https://sc.chinaz.com/tag_tupian/baqi.html'\n",
    "    else:\n",
    "        url = format(url_template%page)\n",
    "        \n",
    "\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    # 一定要注意编码 出现乱码的时候 就要尝试切换编码方式\n",
    "    response.encoding = 'utf-8'\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    img_list = tree.xpath('//div[@id=\"container\"]/div/div/a/img')\n",
    "    '''\n",
    "    涉及到图片懒加载，可视化区域才加载，这时候要爬取的是伪属性\n",
    "    '''\n",
    "\n",
    "    for img in img_list:\n",
    "        new_url = 'https:' + img.xpath('./@src2')[0]\n",
    "        img_title = img.xpath('./@alt')[0]\n",
    "#         print(new_url)\n",
    "        img_data = requests.get(url = new_url, headers = headers).content\n",
    "        with open(dirName+'\\\\' + img_title + '.jpg', 'wb') as f:\n",
    "            f.write(img_data)\n",
    "    f.close()\n",
    "    \n",
    "    print(str(page)+'\\t over')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战二： 站长素材 下载简历\n",
    "import requests \n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "\n",
    "dirName = 'D:/scrapyfile/jianli'\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "url = 'https://sc.chinaz.com/jianli/tongyong.html'\n",
    "response = requests.get(url = url, headers = headers)\n",
    "response.encoding = 'utf-8'\n",
    "page_text = response.text\n",
    "tree = etree.HTML(page_text)\n",
    "img_list = tree.xpath('//div[@id=\"main\"]/div/div/a')\n",
    "for img in img_list:\n",
    "    page_url = img.xpath('./@href')\n",
    "    img_title = img.xpath('./img/@alt')\n",
    "    img_url = \"https:\" + img.xpath('./img/@src')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战三： 站长素材 脚本下载\n",
    "import requests \n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "\n",
    "dirName = 'D:/scrapyfile/jiaoben'\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "url_template = 'https://sc.chinaz.com/jiaoben/index_%d.html'\n",
    "for page in range(1,6):\n",
    "    if page == 1:\n",
    "        url = 'https://sc.chinaz.com/jiaoben/index.html'\n",
    "    else:\n",
    "        url = format(url_template % page) \n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    response.encoding = 'utf-8'\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    jl_list = tree.xpath('//div[@id=\"container\"]/div/div[2]/a')\n",
    "    for jl in jl_list:\n",
    "        new_page = 'https:' + jl.xpath('./@href')[0]\n",
    "        title = jl.xpath('./@alt')[0]\n",
    "    #     print(title,'\\t',  new_page) \n",
    "        new_response = requests.get(url = new_page, headers = headers)\n",
    "        new_response.encoding = 'utf-8'\n",
    "        new_text = etree.HTML(new_response.text)\n",
    "        file_url = new_text.xpath('//div[@class=\"dian\"]/a/@href')[0]\n",
    "        file = requests.get(url = file_url, headers = headers).content\n",
    "        file_name = dirName + '/' + title + '.rar'\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(file)\n",
    "        f.close()\n",
    "    print(str(page) + '\\t over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
